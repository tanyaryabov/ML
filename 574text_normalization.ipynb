{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "574text_normalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO48L8EFAxv8UgP1NY85W+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyaryabov/ML/blob/master/574text_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmY8gWFR9WPq"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')\r\n",
        "import re\r\n",
        "import string\r\n",
        "!pip install contractions\r\n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzyHiAzy9-Tj"
      },
      "source": [
        "class TextNormalizer:\r\n",
        "    \"\"\"\r\n",
        "    Text Normalizer class normalizes text\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self ):\r\n",
        "        self.stopwords= nltk.corpus.stopwords.words(\"english\")\r\n",
        "    \r\n",
        "    def normalize(self, text, clean=True, rm_stopwords=True, rm_special_chars=True, \r\n",
        "                  expand_conts=True, caseConvert=False, lemmatize=True, stem=True):\r\n",
        "        \"\"\"\r\n",
        "        clean->Boolean: Extracts text from html or xml documents , \r\n",
        "        rm_stopwords->Boolean: Removes stop words, \r\n",
        "        rm_special_chars->Boolean: Removes special characters, \r\n",
        "        expand_conts->Boolean: Expands contractions, \r\n",
        "        caseConvert->Boolean: Converts text to upper/lower case , \r\n",
        "        lemmatize->Boolean: Lemmatizes the text to its root form, \r\n",
        "        stem->Boolean: Stems the text\r\n",
        "        \"\"\"\r\n",
        "        if clean:\r\n",
        "            text=self.cleanText(text)\r\n",
        "        if expand_conts:\r\n",
        "            text=self.expandContractions(text)\r\n",
        "        if lemmatize:\r\n",
        "            text=self.lemmatizeText(text)\r\n",
        "        if stem:\r\n",
        "            text=self.stemText(text)\r\n",
        "        if rm_stopwords:\r\n",
        "            text=self.removeStopwords(text)\r\n",
        "        if rm_special_chars:\r\n",
        "            text=self.removeSpecialChars(text)\r\n",
        "        if caseConvert:\r\n",
        "            text=self.caseConvert(text, True)\r\n",
        "        else:\r\n",
        "            text=self.caseConvert(text, False)\r\n",
        "        return text\r\n",
        "        \r\n",
        "    def caseConvert(self, text, upper=False):\r\n",
        "      if upper:\r\n",
        "        return text.upper()\r\n",
        "      return text.lower()\r\n",
        "    \r\n",
        "    def tokenizeText(self,text):\r\n",
        "        words= nltk.word_tokenize(text)\r\n",
        "        tokens= [word.strip() for word in words]\r\n",
        "        return tokens\r\n",
        "    \r\n",
        "    def removeStopwords(self, text):\r\n",
        "        tokens= self.tokenizeText(text)\r\n",
        "        filter_tokens=[token for token in tokens if token not in self.stopwords]\r\n",
        "        text=' '.join(filter_tokens)\r\n",
        "        return text\r\n",
        "    \r\n",
        "    def removeSpecialChars(self,text):\r\n",
        "        tokens=self.tokenizeText(text)\r\n",
        "        pattern= re.compile('[{}]'.format(re.escape(string.punctuation)))\r\n",
        "        filtered_tokens= filter(None,[pattern.sub('',token)for token in tokens])\r\n",
        "        filtered_text=' '.join(filtered_tokens)\r\n",
        "        return filtered_text\r\n",
        "    \r\n",
        "    def expandContractions(self, text):\r\n",
        "      return contractions.fix(text)\r\n",
        "    \r\n",
        "    def lemmatizeText(self, text):\r\n",
        "        pass\r\n",
        "    \r\n",
        "    def stemText(self, text):\r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcjXU1su10Mu"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nspk47W-wO0",
        "outputId": "5b889b32-faf2-4dc9-e41c-16361fe14d2c"
      },
      "source": [
        "if __name__==\"__main__\":\r\n",
        "  tn= TextNormalizer()\r\n",
        "  #normalized_text= tn.tokenizeText(\"The quick fox couldn't jump over the lazy dog\")\r\n",
        "  #print(normalized_text)\r\n",
        "  #normalized_text=tn.expandContractions(\"The quick fox couldn't jump over the lazy dog\")\r\n",
        "  #print(normalized_text)\r\n",
        "  #normalized_text=tn.removeStopwords(\"The quick fox couldn't jump over the lazy dog\")\r\n",
        "  #print(normalized_text)\r\n",
        "  normalized_text=tn.removeSpecialChars(\"The quick fox ! couldn't jump over the lazy dog\")\r\n",
        "  print(normalized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The quick fox could nt jump over the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}